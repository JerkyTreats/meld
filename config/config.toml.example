# Merkle Configuration File
#
# This is an example configuration file for the Merkle filesystem state management system.
# Copy this file to `config/config.toml` or `.merkle/config.toml` and customize it for your needs.
#
# Configuration is loaded in the following order (highest to lowest precedence):
# 1. Environment variables (MERKLE_* prefix)
# 2. Environment-specific files (config/production.toml, config/development.toml)
# 3. Base configuration file (config/config.toml or .merkle/config.toml)
# 4. Default values

[system]
# Default workspace root (if not specified)
default_workspace_root = "."

[system.storage]
# Path to node record store (relative to workspace root)
store_path = ".merkle/store"
# Path to frame storage (relative to workspace root)
frames_path = ".merkle/frames"

# ============================================================================
# Model Provider Configurations
# ============================================================================
# Define model providers that agents can use. Each provider has a unique name
# and configuration. API keys can be set here or via environment variables.
#
# IMPORTANT: The provider name is the part after "providers." in the table name.
# For example, [providers.openai-gpt4] creates a provider with name "openai-gpt4".
# Agents reference providers using this name in their provider_name field.

[providers.openai-gpt4]
provider_type = "openai"
model = "gpt-4"
# API key can be set here or via OPENAI_API_KEY environment variable
# api_key = "sk-..."
endpoint = null
[providers.openai-gpt4.default_options]
temperature = 0.7
max_tokens = 2000
top_p = 0.9

[providers.openai-gpt35]
provider_type = "openai"
model = "gpt-3.5-turbo"
# api_key loaded from OPENAI_API_KEY environment variable
endpoint = null
[providers.openai-gpt35.default_options]
temperature = 0.8
max_tokens = 1500

[providers.anthropic-claude]
provider_type = "anthropic"
model = "claude-3-opus-20240229"
# API key can be set here or via ANTHROPIC_API_KEY environment variable
# api_key = "sk-ant-..."
[providers.anthropic-claude.default_options]
temperature = 0.8
max_tokens = 2000

[providers.local-ollama]
provider_type = "ollama"
model = "llama2"
# Default endpoint is http://localhost:11434
endpoint = "http://localhost:11434"
[providers.local-ollama.default_options]
temperature = 0.8
max_tokens = 1500

[providers.local-custom]
provider_type = "local"
model = "custom-model"
# Full endpoint URL for OpenAI-compatible API
endpoint = "http://localhost:8080/v1"
# Optional API key if your local server requires authentication
# api_key = "local-key"
[providers.local-custom.default_options]
temperature = 0.7
max_tokens = 2000

# ============================================================================
# Agent Configurations
# ============================================================================
# Define agents that can interact with the context engine. Each agent has:
# - agent_id: Unique identifier
# - role: Reader, Writer, or Synthesis
# - system_prompt: Behavior-defining prompt for LLM operations
# - provider_name: Reference to a provider defined above (must match the provider
#                  name exactly - the part after "providers." in the table name)
# - completion_options: Optional override of provider default options
# - metadata: Optional key-value pairs for agent-specific settings

[agents.code-analyzer]
agent_id = "code-analyzer"
role = "Writer"
system_prompt = """
You are a code analysis assistant. Your role is to analyze code files and generate
comprehensive analysis frames that describe the code's structure, purpose, and key
characteristics. Focus on accuracy and clarity.
"""
provider_name = "openai-gpt4"
[agents.code-analyzer.completion_options]
temperature = 0.5
max_tokens = 2000

[agents.documentation-generator]
agent_id = "doc-generator"
role = "Writer"
system_prompt = """
You are a documentation generation assistant. Generate clear, concise documentation
frames that explain code functionality, APIs, and usage patterns. Focus on clarity
and completeness.
"""
provider_name = "openai-gpt35"

[agents.synthesis-agent]
agent_id = "synthesis-agent"
role = "Synthesis"
system_prompt = """
You are a context synthesis assistant. Your role is to combine context frames from
child nodes into coherent branch-level summaries. Maintain accuracy and preserve
key information from child contexts.
"""
provider_name = "openai-gpt4"

[agents.reader-agent]
agent_id = "reader-agent"
role = "Reader"
# Reader agents don't need providers or system prompts
# They can only read context, not generate frames

[agents.local-dev-agent]
agent_id = "local-dev-agent"
role = "Writer"
system_prompt = """
You are a development assistant. Help with code analysis and generation tasks.
Use local models for faster iteration during development.
"""
provider_name = "local-ollama"
[agents.local-dev-agent.metadata]
environment = "development"
preferred_temperature = "0.7"

# ============================================================================
# Watch Mode Configuration
# ============================================================================
# Configuration for watch mode daemon and automatic frame generation

[watch]
# Enable automatic LLM-based frame generation
auto_generate_frames = true

# Frame generation queue configuration
[watch.generation]
# Maximum concurrent generations per agent
max_concurrent_per_agent = 3

# Batch size for processing requests
batch_size = 50

# Maximum retry attempts per request
max_retry_attempts = 3

# Delay between retries (milliseconds)
retry_delay_ms = 1000

# Rate limit: minimum delay between requests per agent (milliseconds)
# Set to null to disable rate limiting
rate_limit_ms = 100

# Maximum queue size (prevents memory exhaustion)
max_queue_size = 10000

# Number of worker tasks per agent
workers_per_agent = 2
